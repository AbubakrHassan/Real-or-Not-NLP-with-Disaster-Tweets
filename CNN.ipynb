{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "import urllib\n",
    "np.random.seed(100)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten,Dropout,Conv1D,Input,Embedding, GlobalMaxPool1D, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mean_squared_error\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath(\"/home/aims/Downloads/glove.twitter.27B/glove.twitter.27B.100d.txt\")\n",
    "tmp_file = get_tmpfile(\"glove_to_w2v.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size=  1193514  words\n"
     ]
    }
   ],
   "source": [
    "wv = model\n",
    "words = list(wv.wv.vocab.keys())\n",
    "word_to_index, index_to_word = dict(),dict()\n",
    "for i,word in enumerate(words):\n",
    "    word_to_index[word]=i\n",
    "    index_to_word[i]=word\n",
    "print(\"vocabulary size= \",len(words),\" words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(words,wv_model,max_length=35):\n",
    "    words = words.lower().split()\n",
    "    vectors = [wv_model[word] for word in words]\n",
    "    return append_zeros(vectors,max_length)\n",
    "def append_zeros(words,max_length):\n",
    "    for i in range(max_length-len(words)):\n",
    "        words.append(np.zeros(300))\n",
    "    return np.array(words)\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len),dtype=int)\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "            if j>=max_len:\n",
    "                break\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path,testing=False):\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    data[\"keyword\"].replace(np.nan,\"-\",inplace=True)\n",
    "    data[\"keyword\"] = data[\"keyword\"].apply(lambda x:urllib.parse.unquote(x))\n",
    "    \n",
    "    data[\"location\"].replace(np.nan,\"-\",inplace=True)\n",
    "    data[\"location\"] = data[\"location\"].apply(lambda x: re.sub(\"[^-a-zA-Z\\s]\",\"\",x))\n",
    "    \n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"[^\\w\\s#'_]\",\"\",x)).apply(lambda x:x.lower())\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: \" \".join([a for a in re.split(\"([#$])\",x) if len(a)!=0]))\n",
    "    \n",
    "    \n",
    "    new_data = pd.DataFrame()\n",
    "    new_data[\"id\"] = data[\"id\"]\n",
    "    \n",
    "    new_data[\"text\"] = data[\"keyword\"] + \" \" + data[\"location\"]+ \" \" + data[\"text\"]\n",
    "    \n",
    "    \n",
    "    if not testing:\n",
    "        new_data[\"target\"] = data[\"target\"]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"hello\"].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "                                            \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer\n",
    "\n",
    "def CNN(input_shape, dropout_prob,n_h):\n",
    "    input_data = Input(shape=input_shape)\n",
    "    embedding_layer = pretrained_embedding_layer(wv,word_to_index)(input_data)\n",
    "    X = LSTM(n_h,return_sequences=True)(embedding_layer)\n",
    "    X = Dropout(dropout_prob)(X)\n",
    "    X = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(X)\n",
    "    X = GlobalMaxPool1D()(X)\n",
    "    \n",
    "    X = Dense(1)(X)\n",
    "    X = Activation(\"sigmoid\")(X)\n",
    "           \n",
    "    model = Model(inputs=input_data, outputs=X)\n",
    "    return model\n",
    "model = CNN((MAX_LENGTH,), 0.5, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 35, 100)           119351500 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 35, 128)           117248    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 33, 250)           96250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 119,565,249\n",
      "Trainable params: 213,749\n",
      "Non-trainable params: 119,351,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>- - our deeds are the reason of this  # earthq...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>- - forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>- - all residents asked to 'shelter in place' ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>- - 13000 people receive  # wildfires evacuati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>- - just got sent this photo from ruby  # alas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1  - - our deeds are the reason of this  # earthq...       1\n",
       "1   4          - - forest fire near la ronge sask canada       1\n",
       "2   5  - - all residents asked to 'shelter in place' ...       1\n",
       "3   6  - - 13000 people receive  # wildfires evacuati...       1\n",
       "4   7  - - just got sent this photo from ruby  # alas...       1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = \"./data/train.csv\"\n",
    "train_data = read_data(train_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data[\"text\"], train_data[\"target\"], test_size=0.33, random_state=42)\n",
    "X_train = sentences_to_indices(np.array(X_train.values),word_to_index,MAX_LENGTH)\n",
    "X_test = sentences_to_indices(np.array(X_test.values),word_to_index,MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=mean_squared_error,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20),ModelCheckpoint(\"best_model.h5\",monitor=\"val_loss\",mode=\"min\",save_best_only=True,verbose=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5100 samples, validate on 2513 samples\n",
      "Epoch 1/100\n",
      "5100/5100 [==============================] - 6s 1ms/step - loss: 0.1712 - accuracy: 0.7453 - val_loss: 0.1449 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14486, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "5100/5100 [==============================] - 4s 756us/step - loss: 0.1414 - accuracy: 0.8002 - val_loss: 0.1405 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14486 to 0.14052, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "5100/5100 [==============================] - 4s 763us/step - loss: 0.1354 - accuracy: 0.8118 - val_loss: 0.1570 - val_accuracy: 0.7672\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14052\n",
      "Epoch 4/100\n",
      "5100/5100 [==============================] - 4s 756us/step - loss: 0.1301 - accuracy: 0.8212 - val_loss: 0.1377 - val_accuracy: 0.8162\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.14052 to 0.13773, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "5100/5100 [==============================] - 4s 825us/step - loss: 0.1256 - accuracy: 0.8294 - val_loss: 0.1362 - val_accuracy: 0.8181\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13773 to 0.13616, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "5100/5100 [==============================] - 4s 771us/step - loss: 0.1193 - accuracy: 0.8435 - val_loss: 0.1362 - val_accuracy: 0.8130\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13616\n",
      "Epoch 7/100\n",
      "5100/5100 [==============================] - 4s 812us/step - loss: 0.1113 - accuracy: 0.8525 - val_loss: 0.1425 - val_accuracy: 0.8102\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13616\n",
      "Epoch 8/100\n",
      "5100/5100 [==============================] - 4s 823us/step - loss: 0.1047 - accuracy: 0.8608 - val_loss: 0.1432 - val_accuracy: 0.8126\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13616\n",
      "Epoch 9/100\n",
      "5100/5100 [==============================] - 4s 824us/step - loss: 0.0981 - accuracy: 0.8759 - val_loss: 0.1492 - val_accuracy: 0.7982\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13616\n",
      "Epoch 10/100\n",
      "5100/5100 [==============================] - 4s 820us/step - loss: 0.0927 - accuracy: 0.8808 - val_loss: 0.1665 - val_accuracy: 0.7768\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13616\n",
      "Epoch 11/100\n",
      "5100/5100 [==============================] - 4s 833us/step - loss: 0.0862 - accuracy: 0.8922 - val_loss: 0.1555 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.13616\n",
      "Epoch 12/100\n",
      "5100/5100 [==============================] - 4s 877us/step - loss: 0.0775 - accuracy: 0.9059 - val_loss: 0.1575 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13616\n",
      "Epoch 13/100\n",
      "5100/5100 [==============================] - 4s 866us/step - loss: 0.0685 - accuracy: 0.9175 - val_loss: 0.1602 - val_accuracy: 0.8078\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13616\n",
      "Epoch 14/100\n",
      "5100/5100 [==============================] - 4s 877us/step - loss: 0.0637 - accuracy: 0.9255 - val_loss: 0.1657 - val_accuracy: 0.7939\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13616\n",
      "Epoch 15/100\n",
      "5100/5100 [==============================] - 4s 880us/step - loss: 0.0591 - accuracy: 0.9310 - val_loss: 0.1655 - val_accuracy: 0.8006\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13616\n",
      "Epoch 16/100\n",
      "5100/5100 [==============================] - 4s 875us/step - loss: 0.0540 - accuracy: 0.9361 - val_loss: 0.1760 - val_accuracy: 0.7883\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.13616\n",
      "Epoch 17/100\n",
      "5100/5100 [==============================] - 4s 882us/step - loss: 0.0502 - accuracy: 0.9412 - val_loss: 0.1681 - val_accuracy: 0.7963\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.13616\n",
      "Epoch 18/100\n",
      "5100/5100 [==============================] - 4s 874us/step - loss: 0.0472 - accuracy: 0.9445 - val_loss: 0.1873 - val_accuracy: 0.7680\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.13616\n",
      "Epoch 19/100\n",
      "5100/5100 [==============================] - 4s 874us/step - loss: 0.0483 - accuracy: 0.9422 - val_loss: 0.1709 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.13616\n",
      "Epoch 20/100\n",
      "5100/5100 [==============================] - 5s 948us/step - loss: 0.0409 - accuracy: 0.9545 - val_loss: 0.1739 - val_accuracy: 0.7979\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.13616\n",
      "Epoch 21/100\n",
      "5100/5100 [==============================] - 5s 915us/step - loss: 0.0370 - accuracy: 0.9592 - val_loss: 0.1756 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.13616\n",
      "Epoch 22/100\n",
      "5100/5100 [==============================] - 5s 888us/step - loss: 0.0381 - accuracy: 0.9571 - val_loss: 0.1778 - val_accuracy: 0.7967\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.13616\n",
      "Epoch 23/100\n",
      "5100/5100 [==============================] - 5s 900us/step - loss: 0.0375 - accuracy: 0.9598 - val_loss: 0.1851 - val_accuracy: 0.7875\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.13616\n",
      "Epoch 24/100\n",
      "5100/5100 [==============================] - 5s 896us/step - loss: 0.0385 - accuracy: 0.9559 - val_loss: 0.1837 - val_accuracy: 0.7851\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.13616\n",
      "Epoch 25/100\n",
      "5100/5100 [==============================] - 6s 1ms/step - loss: 0.0365 - accuracy: 0.9596 - val_loss: 0.1868 - val_accuracy: 0.7847\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.13616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa6e9b0ff60>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32,validation_data=(X_test, y_test),shuffle=True,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
