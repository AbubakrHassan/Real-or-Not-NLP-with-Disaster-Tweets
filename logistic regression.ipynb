{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "import urllib\n",
    "np.random.seed(100)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten,Dropout,Input,Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import mean_squared_error\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath(\"/home/aims/Downloads/glove.twitter.27B/glove.twitter.27B.100d.txt\")\n",
    "tmp_file = get_tmpfile(\"glove_to_w2v.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aims/Documents/Git/Real-or-Not-NLP-with-Disaster-Tweets/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size=  1193514  words\n"
     ]
    }
   ],
   "source": [
    "wv = model\n",
    "words = list(wv.wv.vocab.keys())\n",
    "word_to_index, index_to_word = dict(),dict()\n",
    "for i,word in enumerate(words):\n",
    "    word_to_index[word]=i\n",
    "    index_to_word[i]=word\n",
    "print(\"vocabulary size= \",len(words),\" words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(words,wv_model,max_length=35):\n",
    "    words = words.lower().split()\n",
    "    vectors = [wv_model[word] for word in words]\n",
    "    return append_zeros(vectors,max_length)\n",
    "def append_zeros(words,max_length):\n",
    "    for i in range(max_length-len(words)):\n",
    "        words.append(np.zeros(300))\n",
    "    return np.array(words)\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len),dtype=int)\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "            j = j+1\n",
    "            if j>=max_len:\n",
    "                break\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path,testing=False):\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    data[\"keyword\"].replace(np.nan,\"-\",inplace=True)\n",
    "    data[\"keyword\"] = data[\"keyword\"].apply(lambda x:urllib.parse.unquote(x))\n",
    "    \n",
    "    data[\"location\"].replace(np.nan,\"-\",inplace=True)\n",
    "    data[\"location\"] = data[\"location\"].apply(lambda x: re.sub(\"[^-a-zA-Z\\s]\",\"\",x))\n",
    "    \n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"[^\\w\\s#'_]\",\"\",x)).apply(lambda x:x.lower())\n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: \" \".join([a for a in re.split(\"([#$])\",x) if len(a)!=0]))\n",
    "    \n",
    "    \n",
    "    new_data = pd.DataFrame()\n",
    "    new_data[\"id\"] = data[\"id\"]\n",
    "    \n",
    "    new_data[\"text\"] = data[\"keyword\"] + \" \" + data[\"location\"]+ \" \" + data[\"text\"]\n",
    "    \n",
    "    \n",
    "    if not testing:\n",
    "        new_data[\"target\"] = data[\"target\"]\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 35\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"hello\"].shape[0]\n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "                                            \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer\n",
    "\n",
    "def logistic_regression(input_shape, dropout_prob,n_h):\n",
    "    input_data = Input(shape=input_shape)\n",
    "    embedding_layer = pretrained_embedding_layer(wv,word_to_index)(input_data)\n",
    "    X = Flatten()(embedding_layer)\n",
    "    X = Dense(n_h,activation=\"tanh\")(X)\n",
    "    X = Dropout(dropout_prob)(X)\n",
    "    X = Dense(1)(X)\n",
    "    X = Activation(\"sigmoid\")(X)\n",
    "           \n",
    "    model = Model(inputs=input_data, outputs=X)\n",
    "    return model\n",
    "model = logistic_regression((MAX_LENGTH,), 0.9, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 35, 100)           119351500 \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3500)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               448128    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 119,799,757\n",
      "Trainable params: 448,257\n",
      "Non-trainable params: 119,351,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>- - our deeds are the reason of this  # earthq...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>- - forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>- - all residents asked to 'shelter in place' ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>- - 13000 people receive  # wildfires evacuati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>- - just got sent this photo from ruby  # alas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1  - - our deeds are the reason of this  # earthq...       1\n",
       "1   4          - - forest fire near la ronge sask canada       1\n",
       "2   5  - - all residents asked to 'shelter in place' ...       1\n",
       "3   6  - - 13000 people receive  # wildfires evacuati...       1\n",
       "4   7  - - just got sent this photo from ruby  # alas...       1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = \"./data/train.csv\"\n",
    "train_data = read_data(train_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data[\"text\"], train_data[\"target\"], test_size=0.33, random_state=42)\n",
    "X_train = sentences_to_indices(np.array(X_train.values),word_to_index,MAX_LENGTH)\n",
    "X_test = sentences_to_indices(np.array(X_test.values),word_to_index,MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=mean_squared_error,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(\n",
    "                    monitor='val_loss'\n",
    "                    ,patience=10\n",
    "                    ),\n",
    "             ModelCheckpoint(\n",
    "                     \"best_model.h5\"\n",
    "                     ,monitor=\"val_loss\"\n",
    "                     ,mode=\"min\"\n",
    "                     ,save_best_only=True\n",
    "                     ,verbose=True\n",
    "                    )\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5100 samples, validate on 2513 samples\n",
      "Epoch 1/100\n",
      "5100/5100 [==============================] - 1s 191us/step - loss: 0.3739 - accuracy: 0.5388 - val_loss: 0.2246 - val_accuracy: 0.6407\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22460, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "5100/5100 [==============================] - 1s 161us/step - loss: 0.3352 - accuracy: 0.5751 - val_loss: 0.1773 - val_accuracy: 0.7453\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22460 to 0.17728, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "5100/5100 [==============================] - 1s 146us/step - loss: 0.3040 - accuracy: 0.6012 - val_loss: 0.1652 - val_accuracy: 0.7672\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17728 to 0.16516, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "5100/5100 [==============================] - 1s 147us/step - loss: 0.2693 - accuracy: 0.6457 - val_loss: 0.1732 - val_accuracy: 0.7624\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16516\n",
      "Epoch 5/100\n",
      "5100/5100 [==============================] - 1s 160us/step - loss: 0.2387 - accuracy: 0.6751 - val_loss: 0.1539 - val_accuracy: 0.7875\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16516 to 0.15392, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "5100/5100 [==============================] - 1s 149us/step - loss: 0.2268 - accuracy: 0.6892 - val_loss: 0.1506 - val_accuracy: 0.7990\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15392 to 0.15063, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "5100/5100 [==============================] - 1s 147us/step - loss: 0.2093 - accuracy: 0.7027 - val_loss: 0.1535 - val_accuracy: 0.7871\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.15063\n",
      "Epoch 8/100\n",
      "5100/5100 [==============================] - 1s 148us/step - loss: 0.2004 - accuracy: 0.7178 - val_loss: 0.1473 - val_accuracy: 0.7935\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15063 to 0.14728, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "5100/5100 [==============================] - 1s 150us/step - loss: 0.1864 - accuracy: 0.7324 - val_loss: 0.1470 - val_accuracy: 0.7986\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.14728 to 0.14696, saving model to best_model.h5\n",
      "Epoch 10/100\n",
      "5100/5100 [==============================] - 1s 163us/step - loss: 0.1863 - accuracy: 0.7324 - val_loss: 0.1479 - val_accuracy: 0.8006\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14696\n",
      "Epoch 11/100\n",
      "5100/5100 [==============================] - 1s 152us/step - loss: 0.1792 - accuracy: 0.7447 - val_loss: 0.1438 - val_accuracy: 0.8042\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.14696 to 0.14375, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "5100/5100 [==============================] - 1s 156us/step - loss: 0.1745 - accuracy: 0.7508 - val_loss: 0.1500 - val_accuracy: 0.7919\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14375\n",
      "Epoch 13/100\n",
      "5100/5100 [==============================] - 1s 153us/step - loss: 0.1761 - accuracy: 0.7476 - val_loss: 0.1432 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.14375 to 0.14317, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "5100/5100 [==============================] - 1s 150us/step - loss: 0.1694 - accuracy: 0.7590 - val_loss: 0.1408 - val_accuracy: 0.8094\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14317 to 0.14082, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "5100/5100 [==============================] - 1s 169us/step - loss: 0.1728 - accuracy: 0.7569 - val_loss: 0.1429 - val_accuracy: 0.8082\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14082\n",
      "Epoch 16/100\n",
      "5100/5100 [==============================] - 1s 153us/step - loss: 0.1715 - accuracy: 0.7598 - val_loss: 0.1400 - val_accuracy: 0.8118\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.14082 to 0.14003, saving model to best_model.h5\n",
      "Epoch 17/100\n",
      "5100/5100 [==============================] - 1s 147us/step - loss: 0.1686 - accuracy: 0.7596 - val_loss: 0.1414 - val_accuracy: 0.8074\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14003\n",
      "Epoch 18/100\n",
      "5100/5100 [==============================] - 1s 150us/step - loss: 0.1685 - accuracy: 0.7618 - val_loss: 0.1409 - val_accuracy: 0.8018\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14003\n",
      "Epoch 19/100\n",
      "5100/5100 [==============================] - 1s 154us/step - loss: 0.1695 - accuracy: 0.7590 - val_loss: 0.1421 - val_accuracy: 0.8050\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.14003\n",
      "Epoch 20/100\n",
      "5100/5100 [==============================] - 1s 147us/step - loss: 0.1635 - accuracy: 0.7708 - val_loss: 0.1413 - val_accuracy: 0.8062\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.14003\n",
      "Epoch 21/100\n",
      "5100/5100 [==============================] - 1s 157us/step - loss: 0.1599 - accuracy: 0.7743 - val_loss: 0.1430 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.14003\n",
      "Epoch 22/100\n",
      "5100/5100 [==============================] - 1s 159us/step - loss: 0.1595 - accuracy: 0.7810 - val_loss: 0.1394 - val_accuracy: 0.8074\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.14003 to 0.13944, saving model to best_model.h5\n",
      "Epoch 23/100\n",
      "5100/5100 [==============================] - 1s 150us/step - loss: 0.1598 - accuracy: 0.7773 - val_loss: 0.1406 - val_accuracy: 0.8082\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.13944\n",
      "Epoch 24/100\n",
      "5100/5100 [==============================] - 1s 146us/step - loss: 0.1610 - accuracy: 0.7761 - val_loss: 0.1391 - val_accuracy: 0.8118\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.13944 to 0.13914, saving model to best_model.h5\n",
      "Epoch 25/100\n",
      "5100/5100 [==============================] - 1s 157us/step - loss: 0.1580 - accuracy: 0.7888 - val_loss: 0.1402 - val_accuracy: 0.8082\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.13914\n",
      "Epoch 26/100\n",
      "5100/5100 [==============================] - 1s 149us/step - loss: 0.1625 - accuracy: 0.7702 - val_loss: 0.1398 - val_accuracy: 0.8098\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.13914\n",
      "Epoch 27/100\n",
      "5100/5100 [==============================] - 1s 151us/step - loss: 0.1592 - accuracy: 0.7824 - val_loss: 0.1435 - val_accuracy: 0.8046\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.13914\n",
      "Epoch 28/100\n",
      "5100/5100 [==============================] - 1s 160us/step - loss: 0.1572 - accuracy: 0.7855 - val_loss: 0.1404 - val_accuracy: 0.8078\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.13914\n",
      "Epoch 29/100\n",
      "5100/5100 [==============================] - 1s 147us/step - loss: 0.1569 - accuracy: 0.7780 - val_loss: 0.1423 - val_accuracy: 0.8078\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.13914\n",
      "Epoch 30/100\n",
      "5100/5100 [==============================] - 1s 157us/step - loss: 0.1566 - accuracy: 0.7824 - val_loss: 0.1378 - val_accuracy: 0.8118\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.13914 to 0.13784, saving model to best_model.h5\n",
      "Epoch 31/100\n",
      "5100/5100 [==============================] - 1s 149us/step - loss: 0.1488 - accuracy: 0.7982 - val_loss: 0.1433 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.13784\n",
      "Epoch 32/100\n",
      "5100/5100 [==============================] - 1s 153us/step - loss: 0.1523 - accuracy: 0.7941 - val_loss: 0.1391 - val_accuracy: 0.8146\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.13784\n",
      "Epoch 33/100\n",
      "5100/5100 [==============================] - 1s 157us/step - loss: 0.1504 - accuracy: 0.7965 - val_loss: 0.1411 - val_accuracy: 0.8022\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.13784\n",
      "Epoch 34/100\n",
      "5100/5100 [==============================] - 1s 145us/step - loss: 0.1573 - accuracy: 0.7863 - val_loss: 0.1390 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.13784\n",
      "Epoch 35/100\n",
      "5100/5100 [==============================] - 1s 159us/step - loss: 0.1480 - accuracy: 0.7988 - val_loss: 0.1402 - val_accuracy: 0.8070\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.13784\n",
      "Epoch 36/100\n",
      "5100/5100 [==============================] - 1s 161us/step - loss: 0.1467 - accuracy: 0.8020 - val_loss: 0.1422 - val_accuracy: 0.8058\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.13784\n",
      "Epoch 37/100\n",
      "5100/5100 [==============================] - 1s 149us/step - loss: 0.1507 - accuracy: 0.7920 - val_loss: 0.1428 - val_accuracy: 0.8042\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.13784\n",
      "Epoch 38/100\n",
      "5100/5100 [==============================] - 1s 160us/step - loss: 0.1465 - accuracy: 0.7976 - val_loss: 0.1418 - val_accuracy: 0.8066\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.13784\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100/5100 [==============================] - 1s 157us/step - loss: 0.1538 - accuracy: 0.7886 - val_loss: 0.1399 - val_accuracy: 0.8134\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.13784\n",
      "Epoch 40/100\n",
      "5100/5100 [==============================] - 1s 151us/step - loss: 0.1467 - accuracy: 0.8045 - val_loss: 0.1402 - val_accuracy: 0.8086\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.13784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f6a6d84ae10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32,validation_data=(X_test, y_test),shuffle=True,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-real-or-not-venv",
   "language": "python",
   "name": "kaggle-real-or-not-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
